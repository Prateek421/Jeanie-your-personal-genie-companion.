# -*- coding: utf-8 -*-
"""Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pCzMJWE8VCChzL0Tg1L2DTZk7okKhR-0
"""

import cv2
import numpy as np
from tensorflow.keras.models import load_model
import time
def face_recog():
    emotions = ["Angry","Disgusted","Fearful","Happy","Neutral","Sad","Surprised"]
    face_model = load_model("engine\\face\\Face_Emotional_analysis.h5")
    face_model.summary()
    import cv2
    import numpy as np
    #from tensorflow.keras.models import load_model
    import time
    cap = cv2.VideoCapture(0)
    start_time = time.time()
    interval = 5
    while True:
        ret, frame= cap.read()
        frame = cv2.resize(frame, (700,400))
        if not ret:
            break
        face_detector = cv2.CascadeClassifier('engine\\face\\haarcascade_frontalface_default.xml')
        #frame_rgb = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)

        num_faces = face_detector.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=5)

        for (x,y,w,h) in num_faces:
            cv2.rectangle(frame,(x,y-50), (x+w, y+h+10), (0,255,0), 4)
            roi_gray_frame = frame[y:y+h, x:x+w]
            #cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)),-1),0)
            cropped_img = cv2.resize(roi_gray_frame, (48, 48))
            cropped_img = np.expand_dims(cropped_img, axis=-1)  # Add channel dimension
            cropped_img = np.expand_dims(cropped_img, axis=0)

            emotion_prediction = face_model.predict(cropped_img)
            maxindex = int(np.argmax(emotion_prediction))
            cv2.putText(frame, emotions[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)
            print(emotions[maxindex])


        end_time = time.time()
        time_ex = end_time - start_time
        if(time_ex>=interval):
            start_time = time.time()
        cv2.imshow('Emotion Detection', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

'''emotions = ["Angry","Disgusted","Fearful","Happy","Neutral","Sad","Surprised"]
face_model = load_model("engine\\face\\Face_Emotional_analysis.h5")
face_model.summary()
import cv2
import numpy as np
    #from tensorflow.keras.models import load_model
import time
cap = cv2.VideoCapture(0)
start_time = time.time()
interval = 5
while True:
    ret, frame= cap.read()
    frame = cv2.resize(frame, (700,400))
    if not ret:
        break
    face_detector = cv2.CascadeClassifier('engine\\face\\haarcascade_frontalface_default.xml')
     #frame_rgb = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)

    num_faces = face_detector.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=5)

    for (x,y,w,h) in num_faces:
        cv2.rectangle(frame,(x,y-50), (x+w, y+h+10), (0,255,0), 4)
        roi_gray_frame = frame[y:y+h, x:x+w]
        #cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)),-1),0)
        cropped_img = cv2.resize(roi_gray_frame, (48, 48))
        cropped_img = np.expand_dims(cropped_img, axis=-1)  # Add channel dimension
        cropped_img = np.expand_dims(cropped_img, axis=0)

        emotion_prediction = face_model.predict(cropped_img)
        maxindex = int(np.argmax(emotion_prediction))
        cv2.putText(frame, emotions[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)
        print(emotions[maxindex])

    end_time = time.time()
    time_ex = end_time - start_time
    if(time_ex>=interval):
        start_time = time.time()
    cv2.imshow('Emotion Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()'''